# -*- coding: utf-8 -*-
"""
ë³´ì´ë‹ˆì¹˜ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ
- ë³´ì´ë‹ˆì¹˜ ë¬¸ìì— ë²ˆí˜¸ ë¶€ì—¬
- n/b ì½”ë“œ ë³€í™˜
- ë‹¤êµ­ì–´ ë‹¨ì–´ì™€ ë§¤ì¹­
- ê³ ê¸‰ ë¹„íŠ¸ ê³„ì‚° ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„
"""

from advanced_nb_calculator import (
    BIT_MAX_NB, BIT_MIN_NB,
    word_nb_unicode_format,
    calculate_similarity,
    cosine_similarity,
    calculate_array_order_and_duplicate,
    word_sim,
    levenshtein,
    identify_language
)

class NBCodeConverter:
    """n/b (ìˆ«ì/ë¹„íŠ¸) ì½”ë“œ ë³€í™˜ê¸°"""
    
    def __init__(self):
        # ë¬¸ì-ë²ˆí˜¸ ë§¤í•‘ í…Œì´ë¸”
        self.char_to_number = {}
        self.number_to_char = {}
        self.next_number = 1
        
    def assign_number(self, char):
        """ë¬¸ìì— ë²ˆí˜¸ í• ë‹¹"""
        if char not in self.char_to_number:
            self.char_to_number[char] = self.next_number
            self.number_to_char[self.next_number] = char
            self.next_number += 1
        return self.char_to_number[char]
    
    def char_to_nb_code(self, char):
        """ë¬¸ìë¥¼ n/b ì½”ë“œë¡œ ë³€í™˜"""
        number = self.assign_number(char)
        # 8ë¹„íŠ¸ ì´ì§„ìˆ˜ë¡œ ë³€í™˜
        binary = format(number, '08b')
        # n/b í˜•ì‹: ìˆ«ì_ë¹„íŠ¸ê°’
        return f"{number}/{binary}"
    
    def text_to_nb_codes(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ n/b ì½”ë“œ ë°°ì—´ë¡œ ë³€í™˜"""
        codes = []
        for char in text:
            if char.strip():  # ê³µë°± ì œì™¸
                code = self.char_to_nb_code(char)
                codes.append({
                    'char': char,
                    'number': self.char_to_number[char],
                    'nb_code': code
                })
        return codes
    
    def get_pattern_signature(self, text):
        """í…ìŠ¤íŠ¸ì˜ íŒ¨í„´ ì‹œê·¸ë‹ˆì²˜ ìƒì„± (ë§¤ì¹­ìš©)"""
        codes = self.text_to_nb_codes(text)
        # ìˆ«ì íŒ¨í„´
        number_pattern = [c['number'] for c in codes]
        # ë¹„íŠ¸ í•©ê³„
        bit_sum = sum(bin(c['number']).count('1') for c in codes)
        return {
            'length': len(codes),
            'pattern': number_pattern,
            'bit_sum': bit_sum,
            'nb_codes': codes
        }


class VoynichAnalyzer:
    """ë³´ì´ë‹ˆì¹˜ ë¬¸ì„œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.converter = NBCodeConverter()
        self.voynich_text = ""
        self.voynich_codes = []
        
    def load_voynich_text(self, text):
        """ë³´ì´ë‹ˆì¹˜ í…ìŠ¤íŠ¸ ë¡œë“œ ë° ë²ˆí˜¸ ë¶€ì—¬"""
        self.voynich_text = text
        print(f"\n=== ë³´ì´ë‹ˆì¹˜ ë¬¸ì„œ ë¶„ì„ ===")
        print(f"ì´ ë¬¸ì ìˆ˜: {len(text)}\n")
        
        # ê° ë¬¸ìì— ë²ˆí˜¸ ë¶€ì—¬ ë° n/b ì½”ë“œ ë³€í™˜
        self.voynich_codes = self.converter.text_to_nb_codes(text)
        
        # ê²°ê³¼ ì¶œë ¥
        print("ë¬¸ìë³„ ë²ˆí˜¸ ë° n/b ì½”ë“œ:")
        for i, item in enumerate(self.voynich_codes, 1):  # ëª¨ë“  ë¬¸ì ì¶œë ¥
            print(f"{i:3d}. '{item['char']}' â†’ ë²ˆí˜¸: {item['number']:3d} â†’ n/b: {item['nb_code']}")
        
        # í†µê³„ ì •ë³´ ê³„ì‚°
        print(f"\n{'='*60}")
        print("ğŸ“Š n/b ì½”ë“œ í†µê³„ ë¶„ì„")
        print(f"{'='*60}")
        self._print_statistics()
        
        return self.voynich_codes
    
    def _print_statistics(self):
        """n/b ì½”ë“œ í†µê³„ ì •ë³´ ì¶œë ¥"""
        if not self.voynich_codes:
            return
        
        # ë²ˆí˜¸ ì¶”ì¶œ
        numbers = [item['number'] for item in self.voynich_codes]
        
        # ê¸°ë³¸ í†µê³„
        max_num = max(numbers)
        min_num = min(numbers)
        avg_num = sum(numbers) / len(numbers)
        
        # ë¹„íŠ¸ ìˆ˜ ê³„ì‚°
        bit_counts = [bin(num).count('1') for num in numbers]
        max_bits = max(bit_counts)
        min_bits = min(bit_counts)
        avg_bits = sum(bit_counts) / len(bit_counts)
        total_bits = sum(bit_counts)
        
        # ì •ê·œí™” ê°’ (0~1 ë²”ìœ„)
        normalized = [(num - min_num) / (max_num - min_num) if max_num != min_num else 0 
                      for num in numbers]
        
        print(f"\nğŸ”¢ ë²ˆí˜¸ í†µê³„:")
        print(f"   ìµœì†Œê°’(MIN): {min_num}")
        print(f"   ìµœëŒ€ê°’(MAX): {max_num}")
        print(f"   í‰ê· ê°’(AVG): {avg_num:.2f}")
        print(f"   ë²”ìœ„(RANGE): {max_num - min_num}")
        
        print(f"\nğŸ’¾ ë¹„íŠ¸ í†µê³„:")
        print(f"   ìµœì†Œ ë¹„íŠ¸ ìˆ˜: {min_bits}")
        print(f"   ìµœëŒ€ ë¹„íŠ¸ ìˆ˜: {max_bits}")
        print(f"   í‰ê·  ë¹„íŠ¸ ìˆ˜: {avg_bits:.2f}")
        print(f"   ì´ ë¹„íŠ¸ í•©ê³„: {total_bits}")
        
        print(f"\nğŸ“ˆ ì •ê·œí™” ê°’ (MIN-MAX Normalization):")
        print(f"   ê³µì‹: (ê°’ - MIN) / (MAX - MIN)")
        for i, (item, norm) in enumerate(zip(self.voynich_codes[:10], normalized[:10]), 1):
            print(f"   {i:2d}. '{item['char']}' (ë²ˆí˜¸:{item['number']:2d}) â†’ ì •ê·œí™”: {norm:.4f}")
        if len(normalized) > 10:
            print(f"   ... (ì´ {len(normalized)}ê°œ)")
        
        print(f"\nğŸ¯ ì „ì²´ í…ìŠ¤íŠ¸ ì‹œê·¸ë‹ˆì²˜:")
        print(f"   ë¬¸ì ìˆ˜: {len(self.voynich_codes)}")
        print(f"   ê³ ìœ  ë¬¸ì: {len(self.converter.char_to_number)}ê°œ")
        print(f"   ìˆ«ì íŒ¨í„´: {numbers[:15]}..." if len(numbers) > 15 else f"   ìˆ«ì íŒ¨í„´: {numbers}")
        print(f"   ë¹„íŠ¸ ì‹œê·¸ë‹ˆì²˜: {total_bits}")
        print(f"   ë³µì¡ë„ ì§€ìˆ˜: {total_bits / len(numbers):.2f}")
        
        # ë¹ˆë„ìˆ˜ ë¶„ì„
        from collections import Counter
        freq = Counter(numbers)
        print(f"\nğŸ“Š ë¹ˆë„ìˆ˜ ë¶„ì„ (ìƒìœ„ 5ê°œ):")
        for num, count in freq.most_common(5):
            char = self.converter.number_to_char[num]
            percentage = (count / len(numbers)) * 100
            print(f"   '{char}' (ë²ˆí˜¸:{num}) â†’ {count}íšŒ ({percentage:.1f}%)")
    
    def get_unique_chars(self):
        """ê³ ìœ  ë¬¸ì ëª©ë¡ ë°˜í™˜"""
        return self.converter.char_to_number
    

class LanguageMatcher:
    """ë‹¤êµ­ì–´ ë‹¨ì–´ ë§¤ì¹­ê¸° (ìµœì í™” ë²„ì „)"""
    
    def __init__(self, voynich_analyzer):
        self.analyzer = voynich_analyzer
        self.converter = voynich_analyzer.converter
        self.language_database = {}
        self.word_cache = {}  # ìºì‹± ì¶”ê°€
        
    def add_language_words(self, language, words):
        """ì–¸ì–´ë³„ ë‹¨ì–´ ì¶”ê°€ (ì‚¬ì „ ê³„ì‚° í¬í•¨)"""
        self.language_database[language] = []
        
        print(f"\n{language}: ë‹¨ì–´ ë¶„ì„ ì¤‘...", end=" ")
        for word in words:
            # ì‚¬ì „ì— ìœ ë‹ˆì½”ë“œì™€ ë¹„íŠ¸ ê°’ ê³„ì‚°
            word_unicode = word_nb_unicode_format(word)
            word_max = BIT_MAX_NB(word_unicode)
            word_min = BIT_MIN_NB(word_unicode)
            
            self.language_database[language].append({
                'word': word,
                'unicode': word_unicode,
                'max': word_max,
                'min': word_min
            })
        
        print(f"{len(words)}ê°œ ì™„ë£Œ")
    
    def find_matches(self, voynich_word, threshold=0.7):
        """ë³´ì´ë‹ˆì¹˜ ë‹¨ì–´ì™€ ë§¤ì¹­ë˜ëŠ” ë‹¨ì–´ë“¤ ì°¾ê¸° (ìµœì í™” + ë¹ ë¥¸ í•„í„°ë§)"""
        # ìºì‹œ í™•ì¸
        if voynich_word in self.word_cache:
            cached = self.word_cache[voynich_word]
            return [m for m in cached if m['similarity'] >= threshold]
        
        # ë³´ì´ë‹ˆì¹˜ ë‹¨ì–´ì˜ ìœ ë‹ˆì½”ë“œ ë°°ì—´ ë° ë¹„íŠ¸ ê°’ ê³„ì‚°
        voynich_unicode = word_nb_unicode_format(voynich_word)
        voynich_max = BIT_MAX_NB(voynich_unicode)
        voynich_min = BIT_MIN_NB(voynich_unicode)
        voynich_len = len(voynich_word)
        
        vec1 = [float(x) for x in voynich_unicode]
        
        matches = []
        
        for language, words in self.language_database.items():
            for word_data in words:
                word = word_data['word']
                
                # ë¹ ë¥¸ í•„í„°ë§: ê¸¸ì´ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ìŠ¤í‚µ
                len_diff = abs(len(word) - voynich_len)
                if len_diff > max(len(word), voynich_len) * 0.5:
                    continue
                
                # ì‚¬ì „ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
                word_unicode = word_data['unicode']
                word_max = word_data['max']
                word_min = word_data['min']
                
                # 1. ë¹„íŠ¸ ê°’ ìœ ì‚¬ë„ (ë¹ ë¥¸ ê³„ì‚°)
                bit_similarity = word_sim(voynich_max, voynich_min, word_max, word_min)
                
                # ë¹ ë¥¸ í•„í„°: ë¹„íŠ¸ ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ìŠ¤í‚µ
                if bit_similarity < 30:
                    continue
                
                # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                vec2 = [float(x) for x in word_unicode]
                cosine_sim = cosine_similarity(vec1, vec2) * 100
                
                # 3. Levenshtein ê±°ë¦¬ (ê°„ë‹¨í•œ ê³„ì‚°)
                max_len = max(len(voynich_word), len(word))
                lev_distance = levenshtein(voynich_word, word)
                lev_similarity = ((max_len - lev_distance) / max_len) * 100 if max_len > 0 else 0
                
                # ê°„ì†Œí™”ëœ ì¢…í•© ìœ ì‚¬ë„ (3ê°€ì§€ë§Œ ì‚¬ìš©)
                final_similarity = (
                    bit_similarity * 0.40 +
                    cosine_sim * 0.40 +
                    lev_similarity * 0.20
                ) / 100
                
                if final_similarity >= threshold:
                    matches.append({
                        'language': language,
                        'word': word,
                        'similarity': final_similarity,
                        'details': {
                            'bit_sim': bit_similarity / 100,
                            'cosine': cosine_sim / 100,
                            'levenshtein': lev_similarity / 100,
                            'voynich_max': voynich_max,
                            'voynich_min': voynich_min,
                            'word_max': word_max,
                            'word_min': word_min,
                        }
                    })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìºì‹œ ì €ì¥
        self.word_cache[voynich_word] = matches
        
        return matches



def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ë³´ì´ë‹ˆì¹˜ ë¬¸ì„œ n/b ì½”ë“œ ë¶„ì„ ì‹œìŠ¤í…œ (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ - í™•ì¥íŒ)")
    print("=" * 80)
    
    # 1. ë³´ì´ë‹ˆì¹˜ ë¶„ì„ê¸° ìƒì„±
    analyzer = VoynichAnalyzer()
    
    # 2. ì‹¤ì œ ë³´ì´ë‹ˆì¹˜ ë¬¸ì¥ (ì—¬ëŸ¬ ë‹¨ì–´ë¡œ êµ¬ì„±)
    voynich_sentence = "fachys ykal ar ataiin shol shory cthres y kor sholdy"
    print(f"\nğŸ“œ ë¶„ì„í•  ë³´ì´ë‹ˆì¹˜ ë¬¸ì¥:")
    print(f"   '{voynich_sentence}'")
    print(f"   ë‹¨ì–´ ìˆ˜: {len(voynich_sentence.split())}ê°œ\n")
    
    analyzer.load_voynich_text(voynich_sentence)
    
    print(f"\nê³ ìœ  ë¬¸ì ì¢…ë¥˜: {len(analyzer.get_unique_chars())}ê°œ")
    print("ê³ ìœ  ë¬¸ì ëª©ë¡:", list(analyzer.get_unique_chars().keys()))
    
    # 3. ì–¸ì–´ ë§¤ì¹­ê¸° ìƒì„±
    matcher = LanguageMatcher(analyzer)
    
    # 4. ëŒ€í­ í™•ì¥ëœ ë‹¤êµ­ì–´ ë‹¨ì–´ ë°ì´í„°ë² ì´ìŠ¤
    print("\n" + "=" * 80)
    print("ğŸ“š ë‹¤êµ­ì–´ ë‹¨ì–´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©")
    print("=" * 80)
    
    # í•œêµ­ì–´ - 100ê°œ ì´ìƒ
    matcher.add_language_words('í•œêµ­ì–´', [
        # ìì—°
        'í•˜ëŠ˜', 'ë•…', 'ë°”ë‹¤', 'ì‚°', 'ê°•', 'ë‚˜ë¬´', 'ê½ƒ', 'í’€', 'ëŒ', 'ë¬¼',
        'ë¶ˆ', 'ë°”ëŒ', 'êµ¬ë¦„', 'ë¹„', 'ëˆˆ', 'í•´', 'ë‹¬', 'ë³„', 'ë¹›', 'ê·¸ë¦¼ì',
        # ë™ë¬¼
        'ë§', 'ì†Œ', 'ì–‘', 'ê°œ', 'ê³ ì–‘ì´', 'ìƒˆ', 'ë¬¼ê³ ê¸°', 'ìš©', 'í˜¸ë‘ì´', 'ì‚¬ì',
        # ì‹ë¬¼
        'ì¥ë¯¸', 'ë°±í•©', 'ì—°ê½ƒ', 'ì†Œë‚˜ë¬´', 'ëŒ€ë‚˜ë¬´', 'ë²„ë“œë‚˜ë¬´', 'ë‹¨í’', 'ì€í–‰',
        # ì¶”ìƒ
        'ì‚¬ë‘', 'í‰í™”', 'ììœ ', 'ì§„ë¦¬', 'ì§€í˜œ', 'ìš©ê¸°', 'í¬ë§', 'ë¯¿ìŒ',
        'ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸', 'ë‘ë ¤ì›€', 'í–‰ë³µ', 'ê³ í†µ', 'ì¦ê±°ì›€', 'ì•„ë¦„ë‹¤ì›€',
        # í–‰ë™
        'ê±·ë‹¤', 'ë›°ë‹¤', 'ë³´ë‹¤', 'ë“£ë‹¤', 'ë§í•˜ë‹¤', 'ë¨¹ë‹¤', 'ìë‹¤', 'ì¼í•˜ë‹¤',
        # ì‹œê°„
        'ì•„ì¹¨', 'ë‚®', 'ì €ë…', 'ë°¤', 'ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸',
        # ë°©í–¥/ìœ„ì¹˜
        'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤', 'ì™¼ìª½', 'ì˜¤ë¥¸ìª½', 'ì•ˆ', 'ë°–', 'ê°€ìš´ë°',
        # ì¸ê°„
        'ì‚¬ëŒ', 'ë‚¨ì', 'ì—¬ì', 'ì•„ì´', 'ë¶€ëª¨', 'ì¹œêµ¬', 'ì™•', 'ì—¬ì™•',
        # ì‹ ì²´
        'ë¨¸ë¦¬', 'ëˆˆ', 'ê·€', 'ì½”', 'ì…', 'ì†', 'ë°œ', 'ì‹¬ì¥', 'ëª¸'
    ])
    
    # ì˜ì–´ - 100ê°œ ì´ìƒ
    matcher.add_language_words('ì˜ì–´', [
        # Nature
        'sky', 'earth', 'sea', 'mountain', 'river', 'tree', 'flower', 'grass', 'stone', 'water',
        'fire', 'wind', 'cloud', 'rain', 'snow', 'sun', 'moon', 'star', 'light', 'shadow',
        # Animals
        'horse', 'cow', 'sheep', 'dog', 'cat', 'bird', 'fish', 'dragon', 'tiger', 'lion',
        # Plants
        'rose', 'lily', 'lotus', 'pine', 'bamboo', 'willow', 'maple', 'ginkgo',
        # Abstract
        'love', 'peace', 'freedom', 'truth', 'wisdom', 'courage', 'hope', 'faith',
        'joy', 'sadness', 'anger', 'fear', 'happiness', 'pain', 'pleasure', 'beauty',
        # Actions
        'walk', 'run', 'see', 'hear', 'speak', 'eat', 'sleep', 'work',
        # Time
        'morning', 'day', 'evening', 'night', 'spring', 'summer', 'autumn', 'winter',
        # Direction
        'up', 'down', 'front', 'back', 'left', 'right', 'inside', 'outside', 'center',
        # Human
        'human', 'man', 'woman', 'child', 'parent', 'friend', 'king', 'queen',
        # Body
        'head', 'eye', 'ear', 'nose', 'mouth', 'hand', 'foot', 'heart', 'body',
        # Common words
        'the', 'and', 'or', 'of', 'in', 'on', 'at', 'to', 'for', 'with'
    ])
    
    # ë¼í‹´ì–´ - 100ê°œ ì´ìƒ
    matcher.add_language_words('ë¼í‹´ì–´', [
        # Natura
        'caelum', 'terra', 'mare', 'mons', 'flumen', 'arbor', 'flos', 'herba', 'lapis', 'aqua',
        'ignis', 'ventus', 'nubes', 'pluvia', 'nix', 'sol', 'luna', 'stella', 'lux', 'umbra',
        # Animalia
        'equus', 'bos', 'ovis', 'canis', 'felis', 'avis', 'piscis', 'draco', 'tigris', 'leo',
        # Plantae
        'rosa', 'lilium', 'lotos', 'pinus', 'arundo', 'salix', 'acer',
        # Abstracta
        'amor', 'pax', 'libertas', 'veritas', 'sapientia', 'fortitudo', 'spes', 'fides',
        'gaudium', 'tristitia', 'ira', 'timor', 'felicitas', 'dolor', 'voluptas', 'pulchritudo',
        # Verba
        'ambulare', 'currere', 'videre', 'audire', 'dicere', 'edere', 'dormire', 'laborare',
        # Tempus
        'mane', 'dies', 'vesper', 'nox', 'ver', 'aestas', 'autumnus', 'hiems',
        # Directio
        'supra', 'infra', 'ante', 'post', 'sinister', 'dexter', 'intus', 'extra',
        # Homo
        'homo', 'vir', 'femina', 'puer', 'parens', 'amicus', 'rex', 'regina',
        # Corpus
        'caput', 'oculus', 'auris', 'nasus', 'os', 'manus', 'pes', 'cor', 'corpus',
        # Herbal/Medicine terms (ë³´ì´ë‹ˆì¹˜ì™€ ê´€ë ¨)
        'herba', 'radix', 'folium', 'semen', 'cortex', 'medicina', 'potio', 'unguentum'
    ])
    
    # 5. ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì„ ë° ë²ˆì—­ ì‹œë„
    print("\n" + "=" * 80)
    print("ğŸ” ë³´ì´ë‹ˆì¹˜ ë¬¸ì¥ ë‹¨ì–´ë³„ ë¶„ì„ ë° ë²ˆì—­ ì‹œë„")
    print("=" * 80)
    
    voynich_words = voynich_sentence.split()
    translated_sentence = []
    detailed_results = []
    
    for idx, vword in enumerate(voynich_words, 1):
        print(f"[{idx}/{len(voynich_words)}] '{vword}' ë¶„ì„ ì¤‘...", end=" ")
        
        matches = matcher.find_matches(vword, threshold=0.30)
        
        if matches:
            best_match = matches[0]
            translated_sentence.append(best_match['word'])
            detailed_results.append({
                'original': vword,
                'translated': best_match['word'],
                'language': best_match['language'],
                'similarity': best_match['similarity'],
                'top3': matches[:3]
            })
            print(f"âœ… {best_match['word']} ({best_match['language']}, {best_match['similarity']:.1%})")
        else:
            translated_sentence.append(f"[{vword}]")
            detailed_results.append({
                'original': vword,
                'translated': f"[{vword}]",
                'language': 'unknown',
                'similarity': 0,
                'top3': []
            })
            print(f"âŒ ë§¤ì¹­ ì‹¤íŒ¨")
    
    # 6. ë²ˆì—­ëœ ë¬¸ì¥ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“ ë²ˆì—­ ê²°ê³¼")
    print("=" * 80)
    print(f"\nì›ë¬¸ (ë³´ì´ë‹ˆì¹˜):")
    print(f"  {voynich_sentence}")
    print(f"\në²ˆì—­ë¬¸ (ë‹¤êµ­ì–´ ì¡°í•©):")
    print(f"  {' '.join(translated_sentence)}")
    
    # 7. ìƒì„¸ ë¶„ì„ ê²°ê³¼
    print(f"\n" + "=" * 80)
    print("ğŸ“Š ë‹¨ì–´ë³„ ìƒì„¸ ë¶„ì„")
    print("=" * 80)
    for result in detailed_results:
        print(f"\n'{result['original']}' â†’ '{result['translated']}'")
        if result['top3']:
            print(f"  ìƒìœ„ í›„ë³´:")
            for i, match in enumerate(result['top3'], 1):
                print(f"    {i}. [{match['language']:^6}] {match['word']:12} ({match['similarity']:.1%}) "
                      f"[ë¹„íŠ¸:{match['details']['bit_sim']:.0%} ì½”ì‚¬ì¸:{match['details']['cosine']:.0%}]")
    
    # 8. ì–¸ì–´ë³„ í†µê³„
    from collections import Counter
    language_stats = Counter()
    for result in detailed_results:
        if result['language'] != 'unknown':
            language_stats[result['language']] += 1
    
    print(f"\n" + "=" * 80)
    print("ğŸ“ˆ ì–¸ì–´ë³„ ë§¤ì¹­ í†µê³„")
    print("=" * 80)
    for lang, count in language_stats.most_common():
        percentage = (count / len(voynich_words)) * 100
        print(f"  â€¢ {lang}: {count}ê°œ ë‹¨ì–´ ({percentage:.1f}%)")
    
    # 9. ê°€ëŠ¥í•œ í•´ì„
    print(f"\n" + "=" * 80)
    print("ğŸ’¡ ê°€ëŠ¥í•œ í•´ì„")
    print("=" * 80)
    print(f"\nì´ ë³´ì´ë‹ˆì¹˜ ë¬¸ì¥ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print(f"  '{' '.join(translated_sentence)}'")
    print(f"\nì£¼ìš” ì–¸ì–´: {language_stats.most_common(1)[0][0] if language_stats else 'N/A'}")
    avg_similarity = sum(r['similarity'] for r in detailed_results) / len(detailed_results) if detailed_results else 0
    print(f"í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.1%}")
    
    print("\n" + "=" * 80)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
